{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Cluster \u00b6 My home Kubernetes (k3s) cluster managed by GitOps (Flux2)","title":"Introduction"},{"location":"#home-cluster","text":"My home Kubernetes (k3s) cluster managed by GitOps (Flux2)","title":"Home Cluster"},{"location":"external-secrets/","text":"external-secrets \u00b6 Note : this document is a work in progress Create secret for External Secrets using AWS Secrets Manager \u00b6 kubectl create secret generic aws-credentials \\ --from-literal = id = \"access-key-id\" \\ --from-literal = key = \"access-secret-key\" \\ --namespace kube-system Create any secret using aws-cli \u00b6 aws secretsmanager create-secret \\ --name namespace/secret-name \\ --secret-string \"secret-data\"","title":"external-secrets"},{"location":"external-secrets/#external-secrets","text":"Note : this document is a work in progress","title":"external-secrets"},{"location":"external-secrets/#create-secret-for-external-secrets-using-aws-secrets-manager","text":"kubectl create secret generic aws-credentials \\ --from-literal = id = \"access-key-id\" \\ --from-literal = key = \"access-secret-key\" \\ --namespace kube-system","title":"Create secret for External Secrets using AWS Secrets Manager"},{"location":"external-secrets/#create-any-secret-using-aws-cli","text":"aws secretsmanager create-secret \\ --name namespace/secret-name \\ --secret-string \"secret-data\"","title":"Create any secret using aws-cli"},{"location":"external-shapshotter/","text":"external-shapshotter \u00b6 Version \u00b6 https://github.com/kubernetes-csi/external-snapshotter/tree/release-3.0","title":"external-shapshotter"},{"location":"external-shapshotter/#external-shapshotter","text":"","title":"external-shapshotter"},{"location":"external-shapshotter/#version","text":"https://github.com/kubernetes-csi/external-snapshotter/tree/release-3.0","title":"Version"},{"location":"flux2/","text":"flux-system \u00b6 Bootstrap Flux \u00b6 flux bootstrap github \\ --version = v0.3.0 \\ --owner = onedr0p \\ --repository = home-cluster \\ --path = cluster \\ --personal \\ --network-policy = false","title":"flux-system"},{"location":"flux2/#flux-system","text":"","title":"flux-system"},{"location":"flux2/#bootstrap-flux","text":"flux bootstrap github \\ --version = v0.3.0 \\ --owner = onedr0p \\ --repository = home-cluster \\ --path = cluster \\ --personal \\ --network-policy = false","title":"Bootstrap Flux"},{"location":"rook-ceph/","text":"rook-ceph \u00b6 https://rook.io/docs/rook/v1.2/ceph-common-issues.html Toolbox \u00b6 kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash Crashes \u00b6 Sometime ceph will report a HEALTH_WARN even when the health is fine, in order to get ceph to report back healthly do the following... ceph crash ls # if you want to read the message ceph crash info <id> # archive crash report ceph crash archive <id> # or, archive all crash reports ceph crash archive-all Migration \u00b6 In your shell... # Scale app to 0 replicas kubectl scale deploy/zigbee2mqtt --replicas 0 -n home # Get RBD image name for the app kubectl get pv/ ( k get pv | grep zigbee2mqtt-data | awk -F ' ' '{print $1}' ) -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' # csi-vol-e4a2e40f-2795-11eb-80c7-2298c6796a25 In another shell tab... kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash # create mount directories mkdir -p /mnt/ { tmp,Data } # mount nfs with backups mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=1048576,wsize=1048576,hard\" 192 .168.1.40:/volume1/Data /mnt/Data # optional list rbds rbd list --pool replicapool rbd map -p replicapool csi-vol-e4a2e40f-2795-11eb-80c7-2298c6796a25 mount /dev/rbd0 /mnt/tmp tar xvf /mnt/Data/backups/zigbee2mqtt.tar.gz -C /mnt/tmp chown -R 568 :568 /mnt/tmp/ umount /mnt/tmp rbd unmap -p replicapool csi-vol-e4a2e40f-2795-11eb-80c7-2298c6796a25","title":"rook-ceph"},{"location":"rook-ceph/#rook-ceph","text":"https://rook.io/docs/rook/v1.2/ceph-common-issues.html","title":"rook-ceph"},{"location":"rook-ceph/#toolbox","text":"kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash","title":"Toolbox"},{"location":"rook-ceph/#crashes","text":"Sometime ceph will report a HEALTH_WARN even when the health is fine, in order to get ceph to report back healthly do the following... ceph crash ls # if you want to read the message ceph crash info <id> # archive crash report ceph crash archive <id> # or, archive all crash reports ceph crash archive-all","title":"Crashes"},{"location":"rook-ceph/#migration","text":"In your shell... # Scale app to 0 replicas kubectl scale deploy/zigbee2mqtt --replicas 0 -n home # Get RBD image name for the app kubectl get pv/ ( k get pv | grep zigbee2mqtt-data | awk -F ' ' '{print $1}' ) -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' # csi-vol-e4a2e40f-2795-11eb-80c7-2298c6796a25 In another shell tab... kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash # create mount directories mkdir -p /mnt/ { tmp,Data } # mount nfs with backups mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=1048576,wsize=1048576,hard\" 192 .168.1.40:/volume1/Data /mnt/Data # optional list rbds rbd list --pool replicapool rbd map -p replicapool csi-vol-e4a2e40f-2795-11eb-80c7-2298c6796a25 mount /dev/rbd0 /mnt/tmp tar xvf /mnt/Data/backups/zigbee2mqtt.tar.gz -C /mnt/tmp chown -R 568 :568 /mnt/tmp/ umount /mnt/tmp rbd unmap -p replicapool csi-vol-e4a2e40f-2795-11eb-80c7-2298c6796a25","title":"Migration"},{"location":"sealed-secrets/","text":"sealed-secrets \u00b6 Note : this document is a work in progress Fetch Sealed Secrets Cert \u00b6 kubeseal --controller-name sealed-secrets --fetch-cert > ./secrets/pub-cert.pem","title":"sealed-secrets"},{"location":"sealed-secrets/#sealed-secrets","text":"Note : this document is a work in progress","title":"sealed-secrets"},{"location":"sealed-secrets/#fetch-sealed-secrets-cert","text":"kubeseal --controller-name sealed-secrets --fetch-cert > ./secrets/pub-cert.pem","title":"Fetch Sealed Secrets Cert"},{"location":"snmp-exporter/","text":"snmp-exporter \u00b6 Retrieve metrics from devices that only support monitoring via SNMP. For now I am usng snmp-exporter for getting metrics from my Cyberpower PDUs (model PDU41001) and my APC UPS (Smart-UPS 1500) Clone and build the snmp-exporter generator \u00b6 sudo apt-get install unzip build-essential libsnmp-dev golang go get github.com/prometheus/snmp_exporter/generator cd ${ GOPATH - $HOME /go } /src/github.com/prometheus/snmp_exporter/generator go build make mibs Update generator.yml \u00b6 Kubernetes configmaps have a max size. I needed to srip out all the other modules. modules : apcups : version : 1 walk : - sysUpTime - interfaces - 1.3.6.1.4.1.318.1.1.1.2 # upsBattery - 1.3.6.1.4.1.318.1.1.1.3 # upsInput - 1.3.6.1.4.1.318.1.1.1.4 # upsOutput - 1.3.6.1.4.1.318.1.1.1.7.2 # upsAdvTest - 1.3.6.1.4.1.318.1.1.1.8.1 # upsCommStatus - 1.3.6.1.4.1.318.1.1.1.12 # upsOutletGroups - 1.3.6.1.4.1.318.1.1.10.2.3.2 # iemStatusProbesTable - 1.3.6.1.4.1.318.1.1.26.8.3 # rPDU2BankStatusTable lookups : - source_indexes : [ upsOutletGroupStatusIndex ] lookup : upsOutletGroupStatusName drop_source_indexes : true - source_indexes : [ iemStatusProbeIndex ] lookup : iemStatusProbeName drop_source_indexes : true overrides : ifType : type : EnumAsInfo rPDU2BankStatusLoadState : type : EnumAsStateSet upsAdvBatteryCondition : type : EnumAsStateSet upsAdvBatteryChargingCurrentRestricted : type : EnumAsStateSet upsAdvBatteryChargerStatus : type : EnumAsStateSet cyberpower : version : 1 walk : - ePDUIdentName - ePDUIdentHardwareRev - ePDUStatusInputVoltage ## input voltage (0.1 volts) - ePDUStatusInputFrequency ## input frequency (0.1 Hertz) - ePDULoadStatusLoad ## load (tenths of Amps) - ePDULoadStatusVoltage ## voltage (0.1 volts) - ePDULoadStatusActivePower ## active power (watts) - ePDULoadStatusApparentPower ## apparent power (VA) - ePDULoadStatusPowerFactor ## power factor of the output (hundredths) - ePDULoadStatusEnergy ## apparent power measured (0.1 kw/h). - ePDUOutletControlOutletName ## The name of the outlet. - ePDUOutletStatusLoad ## Outlet load (tenths of Amps) - ePDUOutletStatusActivePower ## Outlet load (watts) - envirTemperature ## temp expressed (1/10 \u00baF) - envirTemperatureCelsius ## temp expressed (1/10 \u00baF) - envirHumidity ## relative humidity (%) Get the Cyberpower MIB \u00b6 wget https://dl4jz3rbrsfum.cloudfront.net/software/CyberPower_MIB_v2.9.MIB.zip unzip CyberPower_MIB_v2.9.MIB.zip mv CyberPower_MIB_v2.9.MIB mibs/ Generate the snmp.yml \u00b6 This will create a snmp.yml file which will be needed for the configmap for snmp-exporter export MIBDIRS = mibs ./generator generate","title":"snmp-exporter"},{"location":"snmp-exporter/#snmp-exporter","text":"Retrieve metrics from devices that only support monitoring via SNMP. For now I am usng snmp-exporter for getting metrics from my Cyberpower PDUs (model PDU41001) and my APC UPS (Smart-UPS 1500)","title":"snmp-exporter"},{"location":"snmp-exporter/#clone-and-build-the-snmp-exporter-generator","text":"sudo apt-get install unzip build-essential libsnmp-dev golang go get github.com/prometheus/snmp_exporter/generator cd ${ GOPATH - $HOME /go } /src/github.com/prometheus/snmp_exporter/generator go build make mibs","title":"Clone and build the snmp-exporter generator"},{"location":"snmp-exporter/#update-generatoryml","text":"Kubernetes configmaps have a max size. I needed to srip out all the other modules. modules : apcups : version : 1 walk : - sysUpTime - interfaces - 1.3.6.1.4.1.318.1.1.1.2 # upsBattery - 1.3.6.1.4.1.318.1.1.1.3 # upsInput - 1.3.6.1.4.1.318.1.1.1.4 # upsOutput - 1.3.6.1.4.1.318.1.1.1.7.2 # upsAdvTest - 1.3.6.1.4.1.318.1.1.1.8.1 # upsCommStatus - 1.3.6.1.4.1.318.1.1.1.12 # upsOutletGroups - 1.3.6.1.4.1.318.1.1.10.2.3.2 # iemStatusProbesTable - 1.3.6.1.4.1.318.1.1.26.8.3 # rPDU2BankStatusTable lookups : - source_indexes : [ upsOutletGroupStatusIndex ] lookup : upsOutletGroupStatusName drop_source_indexes : true - source_indexes : [ iemStatusProbeIndex ] lookup : iemStatusProbeName drop_source_indexes : true overrides : ifType : type : EnumAsInfo rPDU2BankStatusLoadState : type : EnumAsStateSet upsAdvBatteryCondition : type : EnumAsStateSet upsAdvBatteryChargingCurrentRestricted : type : EnumAsStateSet upsAdvBatteryChargerStatus : type : EnumAsStateSet cyberpower : version : 1 walk : - ePDUIdentName - ePDUIdentHardwareRev - ePDUStatusInputVoltage ## input voltage (0.1 volts) - ePDUStatusInputFrequency ## input frequency (0.1 Hertz) - ePDULoadStatusLoad ## load (tenths of Amps) - ePDULoadStatusVoltage ## voltage (0.1 volts) - ePDULoadStatusActivePower ## active power (watts) - ePDULoadStatusApparentPower ## apparent power (VA) - ePDULoadStatusPowerFactor ## power factor of the output (hundredths) - ePDULoadStatusEnergy ## apparent power measured (0.1 kw/h). - ePDUOutletControlOutletName ## The name of the outlet. - ePDUOutletStatusLoad ## Outlet load (tenths of Amps) - ePDUOutletStatusActivePower ## Outlet load (watts) - envirTemperature ## temp expressed (1/10 \u00baF) - envirTemperatureCelsius ## temp expressed (1/10 \u00baF) - envirHumidity ## relative humidity (%)","title":"Update generator.yml"},{"location":"snmp-exporter/#get-the-cyberpower-mib","text":"wget https://dl4jz3rbrsfum.cloudfront.net/software/CyberPower_MIB_v2.9.MIB.zip unzip CyberPower_MIB_v2.9.MIB.zip mv CyberPower_MIB_v2.9.MIB mibs/","title":"Get the Cyberpower MIB"},{"location":"snmp-exporter/#generate-the-snmpyml","text":"This will create a snmp.yml file which will be needed for the configmap for snmp-exporter export MIBDIRS = mibs ./generator generate","title":"Generate the snmp.yml"},{"location":"velero/","text":"velero \u00b6 Velero is a cluster backup & restore solution. I can also leverage restic to backup persistent volumes to S3 storage buckets. In order to backup and restore a given workload, the following steps should work. Backup \u00b6 A backup should already be created by either a scheduled, or manual backup # create a backup for all apps velero backup create manually-backup-1 --from-schedule velero-daily-backup # create a backup for a single app velero backup create jackett-test-abc --include-namespaces testing --selector \"app.kubernetes.io/instance=jackett-test\" --wait Delete Resources \u00b6 # delete the helmrelease kubectl delete hr jackett-test -n testing # allow the application to redeployed and create the new resources # delete the new resources kubectl delete deployment/jackett-test -n jackett kubectl delete pvc/jackett-test-config Restore \u00b6 velero restore create --from-backup velero-daily-backup-20201120020022 --include-namespaces testing --selector \"app.kubernetes.io/instance=jackett-test\" --wait This should not interfere with the HelmRelease or require scaling helm-operator You don't need to worry about adding labels to the HelmRelease or backing-up the helm secret object","title":"velero"},{"location":"velero/#velero","text":"Velero is a cluster backup & restore solution. I can also leverage restic to backup persistent volumes to S3 storage buckets. In order to backup and restore a given workload, the following steps should work.","title":"velero"},{"location":"velero/#backup","text":"A backup should already be created by either a scheduled, or manual backup # create a backup for all apps velero backup create manually-backup-1 --from-schedule velero-daily-backup # create a backup for a single app velero backup create jackett-test-abc --include-namespaces testing --selector \"app.kubernetes.io/instance=jackett-test\" --wait","title":"Backup"},{"location":"velero/#delete-resources","text":"# delete the helmrelease kubectl delete hr jackett-test -n testing # allow the application to redeployed and create the new resources # delete the new resources kubectl delete deployment/jackett-test -n jackett kubectl delete pvc/jackett-test-config","title":"Delete Resources"},{"location":"velero/#restore","text":"velero restore create --from-backup velero-daily-backup-20201120020022 --include-namespaces testing --selector \"app.kubernetes.io/instance=jackett-test\" --wait This should not interfere with the HelmRelease or require scaling helm-operator You don't need to worry about adding labels to the HelmRelease or backing-up the helm secret object","title":"Restore"}]}